{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Part 1 - Machine Learning Pipelines and Orchestration</h1>\n",
    "\n",
    "<font size=3>\n",
    "The goal of this section is to get to know better what orchestration is and why it is needed in mlops. To that end, we will be using Prefect, an open-source python-based orchestrator.\n",
    "\n",
    "This section follows three main steps : \n",
    "\n",
    "1 - Refactoring of the code from the previous section\n",
    "\n",
    "2 - Creating tasks and workflows with Prefect\n",
    "\n",
    "3 - Scheduling automatic runs of the workflows\n",
    "</font>\n",
    "\n",
    "## Intro\n",
    "\n",
    "### What is Orchestration and why orchestrate ?\n",
    "\n",
    "Orchestration refers to the automation of tasks and processes needed to run software applications or machine learning models. They englobe tasks related to development, deployment, and management.\n",
    "\n",
    "> There are different kinds of orchestration: \n",
    "- **Infrastructre orchestration** : Auto provisionning and management of servers, networks, storage, etc.\n",
    "- **Deployment orchestration** : Auto deployment of code into different environments like, dev, staging, testing, or production.\n",
    "- **Service orchestration** : Auto scaling and management of services/microservices inside applications.\n",
    "- **Workflow orchestration** : Auto building, testing, and deploying pipelines.\n",
    "\n",
    "> For the NYC taxi trip duration prediction, we will be implementing **Workflow orchestration**\n",
    "\n",
    "> Why orchestrate : \n",
    "\n",
    "- Better management: have a central place to manage, auto run and monitor machine learning pipelines\n",
    "- Control dependencies: set dependencies between ml steps and ensure the correct order of execution and error handling\n",
    "- Configuration: set different behavior for pipelines, for example, when a failure happens\n",
    "- Reduce manual intervention\n",
    "- Have the possibility to distribute workload\n",
    "- Have the possibility to integrate other tools\n",
    "\n",
    "> Common Orchestration concepts :\n",
    "\n",
    "- **DAG** : Direct Acyclic Graph, represents (a visualisation of) dependencies between steps in a workflow. Steps are connected in a way that does not form any cycles.\n",
    "\n",
    "- **SCHEDULING** : It is the case where the run of a workflow is initiated by a time requirement. It generally uses interval of time defined by the user\n",
    "\n",
    "- **TRIGGER** : It is the case where the run of a workflow is initiated by an action. E.g : The previous task is complete, A specific task fails, A new data notification is received "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From notebook to Workflows :\n",
    "\n",
    "### Why go from notebooks to python files for production ?\n",
    "\n",
    "A major part of the model development phase is done using jupyter notebooks. \\\n",
    "Once the pure development phase is complete, we must ask ourselves a certain number of questions if we want to move our work into a production environment.\n",
    "- How do we manage inputs, outputs, storage and dependencies?\n",
    "- How to integrate our work to an existing infrastructure?\n",
    "- How to get automatic retraining and predictions?\n",
    "\n",
    "`Jupyter notebooks` are:\n",
    "- easy to use and interactive\n",
    "- they make analysis and visualization easy\n",
    "- they allow to run code in a desired order\n",
    "\n",
    "But have major drawbacks that make them **not suitable for production**. They are:\n",
    "- hard to understand when there are a lot of cells\n",
    "- very hard to integrate in infrastructures\n",
    "- not designed for scalability\n",
    "- not designed for collaboration\n",
    "- not necessarily compatible across different environments\n",
    "- not adapted for debugging\n",
    "\n",
    "To solve the majority of these issues, a good practice is to transfer the work from notebooks to **.py** files (python modules and packages). \\\n",
    "Doing this, we can:\n",
    "- improve organization and code understanding\n",
    "- easily port and run in scripts on other infrastructures\n",
    "- use a Python script manager to ensure consistency and reproducibility across different systems\n",
    "- run in parallel and distribute across different machines\n",
    "- make use of any orchestration tool\n",
    "\n",
    "### Good practices preparing code for production\n",
    "\n",
    "- Refactor code : \n",
    "    - set variables in the notebook as function's arguments\n",
    "    - use clear function names and add docstring\n",
    "    - make entrypoint function to perform specific operations\n",
    "    - use typing hints\n",
    "- Split code into different files\n",
    "- Manage dependencies with an environment manager and a `requirement.txt` file\n",
    "- Use git and Github/GitLab for collaboration\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Setup\n",
    "\n",
    "### 0.1 - Imports and globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "from prefect import task, flow\n",
    "from prefect.deployments import Deployment\n",
    "from prefect.orion.schemas.schedules import (\n",
    "    CronSchedule,\n",
    "    IntervalSchedule,\n",
    ")\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = Path(\"..\")\n",
    "\n",
    "\n",
    "class Config:\n",
    "    TRAIN_DATA = ROOT_PATH / \"data/yellow_tripdata_2021-01.parquet\"\n",
    "    TEST_DATA = ROOT_PATH / \"data/yellow_tripdata_2021-02.parquet\"\n",
    "    INFERENCE_DATA = ROOT_PATH / \"data/yellow_tripdata_2021-03.parquet\"\n",
    "    LOCAL_STORAGE = ROOT_PATH / \"results\"\n",
    "    CATEGORICAL_VARS = [\"PULocationID\", \"DOLocationID\"]\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have not run the previous notebook, please run this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\n",
    "    \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\",\n",
    "    config.TRAIN_DATA,\n",
    ")\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-02.parquet\",\n",
    "    config.TEST_DATA,\n",
    ")\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-03.parquet\",\n",
    "    config.INFERENCE_DATA,\n",
    ");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Refactoring \n",
    "\n",
    "A good rule of thumb when writing functions is to delegate a single task to a single function. When working in a Machine Learning project, we often need to chain several operations one after the other. This implies that we need to combine different functions into a workflow.\n",
    "\n",
    "In this section, we will be refactoring our code to write two workflows:\n",
    "\n",
    "- A preprocessing workflow\n",
    "- A model training workflow\n",
    "\n",
    "### 1.1 - Preprocessing\n",
    "\n",
    "The proprocessing workflow is composed of several sequential steps: \n",
    "- Loading the data\n",
    "- Computing the target\n",
    "- Filtering the outliers\n",
    "- Encoding categorical columns\n",
    "- Vectorizing the inputs\n",
    "\n",
    "We will re-use the functions we defined in the previous step for this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    \"Loads a dataframe from a parquet file.\"\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "\n",
    "def compute_target(\n",
    "    taxi_rides: pd.DataFrame,\n",
    "    pickup_column: str = \"tpep_pickup_datetime\",\n",
    "    dropoff_column: str = \"tpep_dropoff_datetime\",\n",
    ") -> pd.DataFrame:\n",
    "    taxi_rides[\"duration\"] = taxi_rides[dropoff_column] - taxi_rides[pickup_column]\n",
    "    taxi_rides[\"duration\"] = taxi_rides[\"duration\"].dt.total_seconds() / 60\n",
    "    return taxi_rides\n",
    "\n",
    "\n",
    "def filter_outliers(\n",
    "    taxi_rides: pd.DataFrame, min_duration: int = 1, max_duration: int = 60\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Filter out outliers based on the ride duration.\"\"\"\n",
    "    return taxi_rides[taxi_rides[\"duration\"].between(min_duration, max_duration)]\n",
    "\n",
    "\n",
    "def encode_categorical_cols(\n",
    "    taxi_rides: pd.DataFrame, categorical_cols: List[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Encode categorical columns in `taxi_rides` dataframe.\"\"\"\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = config.CATEGORICAL_VARS\n",
    "    taxi_rides[categorical_cols] = (\n",
    "        taxi_rides[categorical_cols].fillna(-1).astype(\"int\").astype(\"str\")\n",
    "    )\n",
    "    return taxi_rides\n",
    "\n",
    "\n",
    "def vectorize_dataframe(\n",
    "    taxi_rides: pd.DataFrame,\n",
    "    categorical_cols: List[str] = None,\n",
    "    dict_vectorizer: DictVectorizer = None,\n",
    "    with_target: bool = True,\n",
    ") -> Dict:\n",
    "    \"\"\"Convert a DataFrame into a sparse matrix and target array, optionally using a pre-fit dictionary.\n",
    "\n",
    "    Args:\n",
    "        taxi_rides (pd.DataFrame): DataFrame to be converted.\n",
    "        dict_vectorizer (DictVectorizer, optional): The DictVectorizer to use. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[csr_matrix, np.ndarray, DictVectorizer]: Tuple containing the sparse matrix representation of the\n",
    "        DataFrame, the target array, and the DictVectorizer used to perform the conversion.\n",
    "    \"\"\"\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = config.CATEGORICAL_VARS\n",
    "    dicts = taxi_rides[categorical_cols].to_dict(orient=\"records\")\n",
    "\n",
    "    target = None\n",
    "    if with_target:\n",
    "        target = taxi_rides[\"duration\"].values\n",
    "\n",
    "    if dict_vectorizer is None:\n",
    "        dict_vectorizer = DictVectorizer()\n",
    "        dict_vectorizer.fit(dicts)\n",
    "\n",
    "    features = dict_vectorizer.transform(dicts)\n",
    "\n",
    "    return {\"x\": features, \"y\": target, \"dv\": dict_vectorizer}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task now is to create a function that will chain all these operations together. This will be a workflow that can be called without going into the details of what kind of processing is done to the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(path: str, dict_vectorizer=None, with_target: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Process the input data to extract features, apply necessary transformations,\n",
    "    and convert the data into a sparse matrix.\n",
    "\n",
    "    Args:\n",
    "    - path (str): Path to the input data file\n",
    "    - dict_vectorizer (DictVectorizer): A scikit-learn DictVectorizer object\n",
    "    - with_target (bool): Indicates if the target variable is to be included in the processed data.\n",
    "        Default is True.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing processed data in a sparse matrix format and target variable if applicable\n",
    "\n",
    "    \"\"\"\n",
    "    df = load_data(path)\n",
    "    if with_target:\n",
    "        df = df.pipe(compute_target).pipe(filter_outliers).pipe(encode_categorical_cols)\n",
    "        return vectorize_dataframe(df, dict_vectorizer=dict_vectorizer)\n",
    "    else:\n",
    "        df1 = encode_categorical_cols(df)\n",
    "        return vectorize_dataframe(\n",
    "            df1, dict_vectorizer=dict_vectorizer, with_target=with_target\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Model training\n",
    "\n",
    "Now we'll do the same for the model training. Let's write functions that will be called by 3 different workflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train: csr_matrix, y_train: np.ndarray) -> LinearRegression:\n",
    "    \"\"\"Train and return a linear regression model.\"\"\"\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(x_train, y_train)\n",
    "    return lr\n",
    "\n",
    "\n",
    "def predict_duration(input_data: csr_matrix, model: LinearRegression) -> np.ndarray:\n",
    "    \"\"\"Predict taxi ride duration using a trained model.\"\"\"\n",
    "    return model.predict(input_data)\n",
    "\n",
    "\n",
    "def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Calculate mean squared error for two arrays.\"\"\"\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "\n",
    "def load_pickle(path: str) -> None:\n",
    "    with open(path, \"rb\") as f:\n",
    "        loaded_obj = pickle.load(f)\n",
    "    return loaded_obj\n",
    "\n",
    "\n",
    "def save_pickle(path: str, obj: dict) -> None:\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following function with your code to train, predict and evaluate : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(x_train, y_train, x_test, y_test) -> dict:\n",
    "    \"\"\"Train model, predict values and calculate error\"\"\"\n",
    "    model = train_model(x_train, y_train)\n",
    "    prediction = predict_duration(x_test, model)\n",
    "    mse = evaluate_model(y_test, prediction)\n",
    "    return {\"model\": model, \"mse\": mse}\n",
    "\n",
    "\n",
    "def complete_ml(\n",
    "    train_path: str,\n",
    "    test_path: str,\n",
    "    save_model: bool = True,\n",
    "    save_dv: bool = True,\n",
    "    local_storage: str = Config.LOCAL_STORAGE,\n",
    ") -> None:\n",
    "    \"\"\"Load, process and save data and model.\n",
    "\n",
    "    Loads data, prepares sparse matrix using `DictVectorizer`, trains model,\n",
    "    makes predictions, calculates error, and saves the model and\n",
    "    `DictVectorizer` in pickle format.\n",
    "\n",
    "    Args:\n",
    "        train_path: Path to training data file.\n",
    "        test_path: Path to testing data file.\n",
    "        save_model: Boolean indicating whether to save the trained model.\n",
    "        save_dict_vectorizer: Boolean indicating whether to save the\n",
    "                              `DictVectorizer` used to process the data.\n",
    "        local_storage: Path to the local storage location where the model and\n",
    "                       `DictVectorizer` should be saved.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(local_storage):\n",
    "        os.makedirs(local_storage)\n",
    "\n",
    "    train_data = process_data(train_path)\n",
    "    test_data = process_data(test_path, dv=train_data[\"dv\"])\n",
    "    model_obj = train_and_predict(\n",
    "        train_data[\"x\"], train_data[\"y\"], test_data[\"x\"], test_data[\"y\"]\n",
    "    )\n",
    "    if save_model:\n",
    "        save_pickle(f\"{local_storage}/model.pickle\", model_obj)\n",
    "    if save_dv:\n",
    "        save_pickle(f\"{local_storage}/dv.pickle\", train_data[\"dv\"])\n",
    "\n",
    "\n",
    "def batch_inference(\n",
    "    input_path, dv=None, model=None, local_storage=Config.LOCAL_STORAGE\n",
    "):\n",
    "    \"\"\"Loads data and predicts ride duration\n",
    "\n",
    "    If a DictVectorizer is not provided, it will be loaded from the local storage. If a model is not provided, it will be\n",
    "    loaded from the local storage. The processed data and the predictions are returned.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to the input data\n",
    "        dv (DictVectorizer): DictVectorizer object to transform data\n",
    "        model (Model): Model object to make predictions\n",
    "        local_storage (str): Path to the local storage\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The predictions for the input data\n",
    "    \"\"\"\n",
    "    if not dv:\n",
    "        dv = load_pickle(f\"{local_storage}/dv.pickle\")\n",
    "    data = process_data(input_path, dv, with_target=False)\n",
    "    if not model:\n",
    "        model = load_pickle(f\"{local_storage}/model.pickle\")[\"model\"]\n",
    "    return predict_duration(data[\"x\"], model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Orchestrators\n",
    "\n",
    "Here are a few popular workflow orchestrators :\n",
    "\n",
    "- Apache Airflow : Open-source platform, used for scheduling and managing workflows. It has a large and active community.\n",
    "\n",
    "- Prefect : Open-source platform, designed to be highly flexible, easily deployable and scalable. It offers a Python API.\n",
    "\n",
    "- Flyte : Open-source platform, unified platform for workflow management across cloud and on-premises environments. Also provides a Python API \n",
    "\n",
    "- AWS Step Functions : Serverless workflow service offered by Amazon Web Services. It supports building and executing workflows with multiple steps.\n",
    "\n",
    "- Zapier : Web-based platform that provides a visual interface for connecting different web applications. It does not require coding knowledge.\n",
    "\n",
    "> You can find [a short Orchestrators Benchmark here](https://miro.medium.com/max/1400/1*b6CAci-A4TfuYwM9coY6nw.webp)\n",
    "\n",
    "### Workflow orchestration with prefect\n",
    "\n",
    "> **Version** : \n",
    "> This module has been created using Prefect 2.7.9\n",
    "\n",
    "#### Main Prefect concepts\n",
    "\n",
    "Prefect uses python to build Jobs using functions decorators. \n",
    "As long as your main workflow function is decorated, any run of such flow becomes observable from the Prefect UI.\n",
    "\n",
    "Basic concepts : \n",
    "- **Tasks in prefect** : They are units of work written in python. A task is a function decorated with the @task Prefect decorator. They can only be called in flows.\n",
    "- **Flows** : They are dags that represents a group of interdependant tasks. \n",
    "- **Engine** : This is what define where to run flows. This is where we manage workload distribution. In this course, we only use local machine.\n",
    "- **State** : They are prefect objects returned by flows. Contain informations about flows and data.\n",
    "\n",
    "Deployment concepts : \n",
    "- **Deployment object** : These are prefect entities that the api can understand for scheduling, auto-runs etc.\n",
    "- **Work queue** : These are created after a deployment has been applied. It lists all the upcomming runs and their state. (scheduled, running ...)\n",
    "- **Agent** : These are responsable of pulling the flows from work queues for run a the right time. \n",
    "\n",
    "### 2.1 - Set up Prefect UI\n",
    "\n",
    "Steps :\n",
    "\n",
    "- Set an API URL for your local server to make sure that your workflow will be tracked by this specific instance : \n",
    "    - `prefect config set PREFECT_API_URL=http://0.0.0.0:4200/api`\n",
    "- Start a local prefect server : `prefect orion start --host 0.0.0.0`\n",
    "\n",
    "Prefect database is stored at `~/.prefect/orion.db`. If you want to reset the database, run `prefect orion database reset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "## Set your prefect UI up (Use a separate terminal)\n",
    "## Do NOT run the command on this cell using \"!\"\n",
    "###################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to configure tasks and flows behavior using arguments in the decorators : \n",
    "- name, tags\n",
    "- version\n",
    "- retries on failure\n",
    "- etc ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(name=\"failure_task\", tags=[\"fails\"], retries=3, retry_delay_seconds=60)\n",
    "def failure():\n",
    "    print(\"running\")\n",
    "    if random.randint(1, 10) % 2 == 0:\n",
    "        raise ValueError(\"bad code\")\n",
    "\n",
    "\n",
    "@flow(name=\"failure_flow\", version=\"1.0\")\n",
    "def test_failure():\n",
    "    failure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_failure()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - First flow : Processing flow\n",
    "\n",
    "Create a processing flow using the functions we defined in section 1 and prefect (copy-paste and decorate): \n",
    " - tasks : all subfunctions of your preprocessing  \n",
    " - flows : your preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(name=\"load_data\", tags=[\"preprocessing\"], retries=2, retry_delay_seconds=60)\n",
    "def load_data(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "\n",
    "@task(name=\"compute_duration\", tags=[\"preprocessing\"])\n",
    "def compute_target(\n",
    "    taxi_rides: pd.DataFrame,\n",
    "    pickup_column: str = \"tpep_pickup_datetime\",\n",
    "    dropoff_column: str = \"tpep_dropoff_datetime\",\n",
    ") -> pd.DataFrame:\n",
    "    taxi_rides[\"duration\"] = taxi_rides[dropoff_column] - taxi_rides[pickup_column]\n",
    "    taxi_rides[\"duration\"] = taxi_rides[\"duration\"].dt.total_seconds() / 60\n",
    "    return taxi_rides\n",
    "\n",
    "\n",
    "@task(name=\"filter_outliers\", tags=[\"preprocessing\"])\n",
    "def filter_outliers(\n",
    "    taxi_rides: pd.DataFrame, min_duration: int = 1, max_duration: int = 60\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Filter out outliers based on the ride duration.\"\"\"\n",
    "    return taxi_rides[taxi_rides[\"duration\"].between(min_duration, max_duration)]\n",
    "\n",
    "\n",
    "@task(name=\"encode_cat_cols\", tags=[\"preprocessing\"])\n",
    "def encode_categorical_cols(\n",
    "    taxi_rides: pd.DataFrame, categorical_cols: List[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Encode categorical columns in `taxi_rides` dataframe.\"\"\"\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = config.CATEGORICAL_VARS\n",
    "    taxi_rides[categorical_cols] = (\n",
    "        taxi_rides[categorical_cols].fillna(-1).astype(\"int\").astype(\"str\")\n",
    "    )\n",
    "    return taxi_rides\n",
    "\n",
    "\n",
    "@task(name=\"vectorize_dataframe\", tags=[\"preprocessing\"])\n",
    "def vectorize_dataframe(\n",
    "    taxi_rides: pd.DataFrame,\n",
    "    categorical_cols: List[str] = None,\n",
    "    dict_vectorizer: DictVectorizer = None,\n",
    "    with_target: bool = True,\n",
    ") -> Dict:\n",
    "    \"\"\"Convert a DataFrame into a sparse matrix and target array, optionally using a pre-fit dictionary.\"\"\"\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = config.CATEGORICAL_VARS\n",
    "    dicts = taxi_rides[categorical_cols].to_dict(orient=\"records\")\n",
    "\n",
    "    target = None\n",
    "    if with_target:\n",
    "        target = taxi_rides[\"duration\"].values\n",
    "\n",
    "    if dict_vectorizer is None:\n",
    "        dict_vectorizer = DictVectorizer()\n",
    "        dict_vectorizer.fit(dicts)\n",
    "\n",
    "    features = dict_vectorizer.transform(dicts)\n",
    "\n",
    "    return {\"x\": features, \"y\": target, \"dv\": dict_vectorizer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flow(name=\"Data processing\", retries=1, retry_delay_seconds=30)\n",
    "def process_data(path: Path, dict_vectorizer=None, with_target: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Load data from a parquet file\n",
    "    Compute target(duration column) and apply threshold filters (optional)\n",
    "    Turn features to sparce matrix\n",
    "    :return The sparce matrix, the target' values and the\n",
    "    dictvectorizer object if needed.\n",
    "    \"\"\"\n",
    "    df = load_data(path)\n",
    "    if with_target:\n",
    "        df = df.pipe(compute_target).pipe(filter_outliers).pipe(encode_categorical_cols)\n",
    "        return vectorize_dataframe(df, dict_vectorizer=dict_vectorizer)\n",
    "    else:\n",
    "        df1 = encode_categorical_cols(df)\n",
    "        return vectorize_dataframe(\n",
    "            df1, dict_vectorizer=dict_vectorizer, with_target=with_target\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = process_data(Config.TRAIN_DATA)\n",
    "\n",
    "res_without_y = process_data(\n",
    "    path=Config.INFERENCE_DATA, dict_vectorizer=res[\"dv\"], with_target=False\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Training flow\n",
    "\n",
    "Replicate what you just did above, but now for the training flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(name=\"Train model\", tags=[\"Model\"])\n",
    "def train_model(x_train: csr_matrix, y_train: np.ndarray) -> LinearRegression:\n",
    "    \"\"\"Train and return a linear regression model\"\"\"\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(x_train, y_train)\n",
    "    return lr\n",
    "\n",
    "\n",
    "@task(name=\"Make prediction\", tags=[\"Model\"])\n",
    "def predict_duration(input_data: csr_matrix, model: LinearRegression) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Use trained linear regression model\n",
    "    to predict target from input data\n",
    "    :return array of predictions\n",
    "    \"\"\"\n",
    "    return model.predict(input_data)\n",
    "\n",
    "\n",
    "@task(name=\"Evaluation\", tags=[\"Model\"])\n",
    "def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Calculate mean squared error for two arrays\"\"\"\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "\n",
    "@task(name=\"Load\", tags=[\"Serialize\"])\n",
    "def load_pickle(path: str):\n",
    "    with open(path, \"rb\") as f:\n",
    "        loaded_obj = pickle.load(f)\n",
    "    return loaded_obj\n",
    "\n",
    "\n",
    "@task(name=\"Save\", tags=[\"Serialize\"])\n",
    "def save_pickle(path: str, obj: dict):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flow(name=\"Model initialisation\")\n",
    "def train_and_predict(x_train, y_train, x_test, y_test) -> dict:\n",
    "    \"\"\"Train model, predict values and calculate error\"\"\"\n",
    "    model = train_model(x_train, y_train)\n",
    "    prediction = predict_duration(x_test, model)\n",
    "    mse = evaluate_model(y_test, prediction)\n",
    "    return {\"model\": model, \"mse\": mse}\n",
    "\n",
    "\n",
    "@flow(name=\"Example Machine learning workflow\", retries=1, retry_delay_seconds=30)\n",
    "def complete_ml(\n",
    "    train_path: Path,\n",
    "    test_path: Path,\n",
    "    save_model: bool = True,\n",
    "    save_dv: bool = True,\n",
    "    local_storage: Path = Config.LOCAL_STORAGE,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load data and prepare sparse matrix (using dictvectorizer) for model training\n",
    "    Train model, make predictions and calculate error\n",
    "    Save model and dictvectorizer to a folder in pickle format\n",
    "    :return none\n",
    "    \"\"\"\n",
    "    if not os.path.exists(local_storage):\n",
    "        os.makedirs(local_storage)\n",
    "\n",
    "    train_data = process_data(train_path)\n",
    "    test_data = process_data(test_path, dict_vectorizer=train_data[\"dv\"])\n",
    "    model_obj = train_and_predict(\n",
    "        train_data[\"x\"], train_data[\"y\"], test_data[\"x\"], test_data[\"y\"]\n",
    "    )\n",
    "    if save_model:\n",
    "        save_pickle(f\"{local_storage}/model.pickle\", model_obj)\n",
    "    if save_dv:\n",
    "        save_pickle(f\"{local_storage}/dv.pickle\", train_data[\"dv\"])\n",
    "\n",
    "\n",
    "@flow(name=\"Batch inference\", retries=1, retry_delay_seconds=30)\n",
    "def batch_inference(\n",
    "    input_path, dv=None, model=None, local_storage=Config.LOCAL_STORAGE\n",
    "):\n",
    "    \"\"\"\n",
    "    Load model and dictvectorizer from folder\n",
    "    Transforms input data with dictvectorizer\n",
    "    Predict values using loaded model\n",
    "    :return array of predictions\n",
    "    \"\"\"\n",
    "    if not dv:\n",
    "        dv = load_pickle(f\"{local_storage}/dv.pickle\")\n",
    "    data = process_data(input_path, dv, with_target=False)\n",
    "    if not model:\n",
    "        model = load_pickle(f\"{local_storage}/model.pickle\")[\"model\"]\n",
    "    return predict_duration(data[\"x\"], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_ml(Config.TRAIN_DATA, Config.TEST_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inference(Config.INFERENCE_DATA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Deployments\n",
    "\n",
    "Prefect deployment objects are instances that are used by the prefect API to understand scheduling requirements. \\\n",
    "A flow can be used in multiple deployment objects, but a deployment object is associated to a unique flow. \\\n",
    "It creates work queues and an agent that manages the runs.\n",
    "\n",
    "There are two types of scheduling that can be used with prefect : \n",
    "- cron scheduling : define runs dates based on a cron expression. e.g. : `\"0 0 * * 0\"` (every sunday at 00:00)\n",
    "- interval scheduling : define runs interval in minutes/seconds/...\n",
    "\n",
    "> ATTENTION: To configure shcedulling with prefect, all your flows and tasks have to be written in a python file. \n",
    "\n",
    "**Step 1 : Copy your tasks/flows in a python file**\n",
    "\n",
    "Schedulings are created using : \n",
    "```\n",
    "Deployment.build_from_flow(\n",
    "    name: <the name of the object>,\n",
    "    version : <optionnal>,\n",
    "    tags : [<optionnal tags>],\n",
    "    schedule: <CronSchedule(...)> / <IntervalSchedule(...)>\n",
    "    apply: True # send to the prefect API\n",
    "    parameters: {...}\n",
    "    entrypoint=f\"<path to your python file .py>:<your flow name>\",\n",
    ")\n",
    "```\n",
    "\n",
    "**Step 2 : Define some scheduling to run the complete ML workflow each sunday and the batch inference at a desired time interval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_deployment_every_sunday = await Deployment.build_from_flow(\n",
    "    name=\"Model training Deployment\",\n",
    "    flow=complete_ml,\n",
    "    version=\"1.0\",\n",
    "    tags=[\"model\"],\n",
    "    schedule=CronSchedule(cron=\"0 0 * * 0\"),\n",
    "    apply=True,\n",
    "    entrypoint=\"/app/lib/prefect_workflows.py:complete_ml\",\n",
    "    parameters={\n",
    "        \"train_path\": Config.TRAIN_DATA.resolve(),\n",
    "        \"test_path\": Config.TEST_DATA.resolve(),\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "inference_deployment_every_minute = await Deployment.build_from_flow(\n",
    "    name=\"Model Inference Deployment\",\n",
    "    flow=batch_inference,\n",
    "    version=\"1.0\",\n",
    "    tags=[\"inference\"],\n",
    "    schedule=IntervalSchedule(interval=60),\n",
    "    apply=True,\n",
    "    entrypoint=\"/app/lib/prefect_workflows.py:batch_inference\",\n",
    "    parameters={\"input_path\": Config.INFERENCE_DATA.resolve()},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prefect agent is needed to pull the works and run the flows at the right time/interval.\n",
    "Start one with : \n",
    "```\n",
    "prefect agent start default\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open Discussion : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Discuss a global vision of how to implement pipelines triggering (by action, not time)\n",
    "- Discuss a way to implement it to the NYC use case. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xhec-mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "69df463de5e93079cb611bfbe23df861107f59c114c35b844e6461f75b971115"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
