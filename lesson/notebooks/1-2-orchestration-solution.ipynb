{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Machine Learning Pipelines and Orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of this section is to get to know better what is orchestration and why it is needed in mlops using Prefect, an open source python based orchestrator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro and Use case Reminder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project is *New York City Taxi trip duration prediction*. \\\n",
    "The goal is to use the available data in order to train a simple machine learning model\n",
    "to predict the trip duration based on **some input that can be available in production environment**.\n",
    "\n",
    "An ultimate goal for this use case can be to predict in real time trips durations (google-maps/waze itinerary like)\n",
    "but for simplicity, in this module, we assume that we need batch prediction. The data for which we need predictions\n",
    "will be stored in a file for ingestion in the trained model.\n",
    "\n",
    "The machine learning phase is mainly constituted by the following steps : \n",
    "- data processing\n",
    "- model training\n",
    "- model evaluation\n",
    "- prediction\n",
    "\n",
    "The data to use for this module can be downloaded from the [TLC Trip Record Data page](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).\n",
    "To complete this module, you will need 03 samples of data :\n",
    "- `sample 1 example` : yellow trip 2021-01 data (to train model)\n",
    "- `sample 2 example` : yellow trip 2021-02 data (to evaluate model)\n",
    "- `sample 3 example` : yellow trip 2021-03 data (for prediction)\n",
    "\n",
    "A simple machine learning implementation can be found in [mlops-crashcourse.ipynb](null).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Orchestration and Why Orchestrate ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orchestration refers to Automation of tasks and processes related to the development, deployment, and management of software applications or machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is different kind of orchestration. \n",
    "- **Infrastructre orchestration** : Auto provisionning and management of servers, networks, storage etc..\n",
    "- **Deployment orchestration** : Auto deployment of code into different environments like, dev, stagging, testing, production ...\n",
    "- **Service orchestration** : Auto scaling and management of services/microservices inside applications \n",
    "- **Workflow orchestration** : Auto building, testing and deploying pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For the NYC taxi trip duration prediction, we will be implementing **Workflow orchestration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Why orchestrate : \n",
    "\n",
    "- Have a central place to manage, auto run and monitor machine learning pipelines\n",
    "- Set dependencies between ml steps and ensure correct order execution and error handling\n",
    "- Configure different behavior for pipelines, for example, what happens on failure\n",
    "- Reduce manual intervention\n",
    "- Have possibility to distribute workload\n",
    "- Have possibility to integrate other tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Common Orchestration concepts :\n",
    "\n",
    "- **DAG** : Direct Acyclic Graph, represents (a visualisation of) dependencies between steps in a workflow. Steps are connected in a way that does not form any cycles.\n",
    "\n",
    "- **SCHEDULING** : It is the case where the run of a workflow is initiated by a time requirement. It generally uses interval of time defined by the user\n",
    "\n",
    "- **TRIGGER** : It is the case where the run of a workflow is initiated by an action. E.g : The previous task is complete, A specific task fails, A new data notification is received "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From notebook to Workflows :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why change notebooks to python files for production ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major part of the model development phase is made by data scientist using jupyter notebooks. \\\n",
    "Once the pure development phase is complete, we must ask ourselves a certain number of questions if we want to move our work into a production environment.\n",
    "- How do we manage inputs, outputs, storage en dependancies ?\n",
    "- How to integrate our work to an existing infrastructure ?\n",
    "- How to get automatic retraining / predictions ?\n",
    "- and there is more ...\n",
    "\n",
    "`Jupyter notebooks` are :\n",
    "- easy to use and interactive\n",
    "- ease analysis and visualizations\n",
    "- allow to run in desired order\n",
    "\n",
    "But have major drawbacks that make them **not production suitable** :\n",
    "- Hard to understand when there is lots of cells\n",
    "- Very hard to integrate in infrastructures\n",
    "- not designed for scalability\n",
    "- not designed for collaboration\n",
    "- Can have compatibility issues in different environments\n",
    "- Debugging ? ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the major part of this issues, a good practice is to transfer the work from notebooks to **.py** files. \\\n",
    "Doing this, we can:\n",
    "- facilitate organization and ease the code understanding\n",
    "- python scripts are easily integrable and runnable in other infrastructures\n",
    "- we can use Python script manager to ensure consistency and reproducibility across different systems\n",
    "- run in parallel and distribute across different machines\n",
    "- They are compatibles with all orchestration tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good practices preparing code for production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Refactor code : \n",
    "    - set variables in the notebook as function's arguments\n",
    "    - use clear functions names and add docstring\n",
    "    - make entrypoint functions to perform specific operations\n",
    "    - use typing\n",
    "- Split code into different files following the use\n",
    "- Manage dependencies with Conda/pyenv and a `requirement.txt` file\n",
    "- Use git and Github/GitLab for collaboration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "from typing import List\n",
    "from scipy.sparse import csr_matrix\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "from prefect import task, flow\n",
    "from prefect.deployments import Deployment\n",
    "from prefect.orion.schemas.schedules import (\n",
    "    CronSchedule,\n",
    "    IntervalSchedule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/app/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = base_dir + \"data/yellow_tripdata_2021-01.parquet\"\n",
    "test_path = base_dir + \"data/yellow_tripdata_2021-02.parquet\"\n",
    "inference_path = base_dir + \"data/yellow_tripdata_2021-03.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\n",
    "    \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\",\n",
    "    train_path,\n",
    ")\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-02.parquet\",\n",
    "    test_path,\n",
    ")\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-03.parquet\",\n",
    "    inference_path,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    TRAIN_DATA = train_path\n",
    "    TEST_DATA = test_path\n",
    "    INFERENCE_DATA = inference_path\n",
    "    LOCAL_STORAGE = base_dir + \"results\"\n",
    "    CATEGORICAL_VARS = [\"PULocationID\", \"DOLocationID\", \"passenger_count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "\n",
    "def compute_target(\n",
    "    df: pd.DataFrame,\n",
    "    pickup_column: str = \"tpep_pickup_datetime\",\n",
    "    dropoff_column: str = \"tpep_dropoff_datetime\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the trip duration in minutes based\n",
    "    on pickup and dropoff time\n",
    "    \"\"\"\n",
    "    df[\"duration\"] = df[dropoff_column] - df[pickup_column]\n",
    "    df[\"duration\"] = df[\"duration\"].dt.total_seconds() / 60\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_outliers(\n",
    "    df: pd.DataFrame, min_duration: int = 1, max_duration: int = 60\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove rows corresponding to negative/zero\n",
    "    and too high target' values from the dataset\n",
    "    \"\"\"\n",
    "    return df[df[\"duration\"].between(min_duration, max_duration)]\n",
    "\n",
    "\n",
    "def encode_categorical_cols(\n",
    "    df: pd.DataFrame, categorical_cols: List[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a Pandas dataframe and a list of categorical\n",
    "    column names, and returns dataframe with\n",
    "    the specified columns converted to categorical data type\n",
    "    \"\"\"\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = Config.CATEGORICAL_VARS\n",
    "    df[categorical_cols] = df[categorical_cols].fillna(-1).astype(\"int\")\n",
    "    df[categorical_cols] = df[categorical_cols].astype(\"str\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_x_y(\n",
    "    df: pd.DataFrame,\n",
    "    categorical_cols: List[str] = None,\n",
    "    dv: DictVectorizer = None,\n",
    "    with_target: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Turns lists of mappings (dicts of feature names to feature values)\n",
    "    into sparse matrices for use with scikit-learn estimators\n",
    "    using Dictvectorizer object.\n",
    "    :return The sparce matrix, the target' values if needed and the\n",
    "    dictvectorizer object.\n",
    "    \"\"\"\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = Config.CATEGORICAL_VARS\n",
    "    dicts = df[categorical_cols].to_dict(orient=\"records\")\n",
    "\n",
    "    y = None\n",
    "    if with_target:\n",
    "        if dv is None:\n",
    "            dv = DictVectorizer()\n",
    "            dv.fit(dicts)\n",
    "        y = df[\"duration\"].values\n",
    "\n",
    "    x = dv.transform(dicts)\n",
    "    return {\"x\": x, \"y\": y, \"dv\": dv}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(path: str, dv=None, with_target: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Load data from a parquet file\n",
    "    Compute target (duration column) and apply threshold filters (optional)\n",
    "    Turn features to sparce matrix\n",
    "    :return The sparce matrix, the target' values and the\n",
    "    dictvectorizer object if needed.\n",
    "    \"\"\"\n",
    "    df = load_data(path)\n",
    "    if with_target:\n",
    "        df1 = compute_target(df)\n",
    "        df2 = filter_outliers(df1)\n",
    "        df3 = encode_categorical_cols(df2)\n",
    "        return extract_x_y(df3, dv=dv)\n",
    "    else:\n",
    "        df1 = encode_categorical_cols(df)\n",
    "        return extract_x_y(df1, dv=dv, with_target=with_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(x_train: csr_matrix, y_train: np.ndarray) -> LinearRegression:\n",
    "    \"\"\"Train and return a linear regression model\"\"\"\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(x_train, y_train)\n",
    "    return lr\n",
    "\n",
    "\n",
    "def predict_duration(input_data: csr_matrix, model: LinearRegression) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Use trained linear regression model\n",
    "    to predict target from input data\n",
    "    :return array of predictions\n",
    "    \"\"\"\n",
    "    return model.predict(input_data)\n",
    "\n",
    "\n",
    "def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Calculate mean squared error for two arrays\"\"\"\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(path: str):\n",
    "    with open(path, \"rb\") as f:\n",
    "        loaded_obj = pickle.load(f)\n",
    "    return loaded_obj\n",
    "\n",
    "\n",
    "def save_pickle(path: str, obj: dict):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(x_train, y_train, x_test, y_test) -> dict:\n",
    "    \"\"\"Train model, predict values and calculate error\"\"\"\n",
    "    model = train_model(x_train, y_train)\n",
    "    prediction = predict_duration(x_test, model)\n",
    "    mse = evaluate_model(y_test, prediction)\n",
    "    return {\"model\": model, \"mse\": mse}\n",
    "\n",
    "\n",
    "def complete_ml(\n",
    "    train_path: str,\n",
    "    test_path: str,\n",
    "    save_model: bool = True,\n",
    "    save_dv: bool = True,\n",
    "    local_storage: str = Config.LOCAL_STORAGE,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load data and prepare sparse matrix (using dictvectorizer) for model training\n",
    "    Train model, make predictions and calculate error\n",
    "    Save model and dictvectorizer to a folder in pickle format\n",
    "    :return none\n",
    "    \"\"\"\n",
    "    if not os.path.exists(local_storage):\n",
    "        os.makedirs(local_storage)\n",
    "\n",
    "    train_data = process_data(train_path)\n",
    "    test_data = process_data(test_path, dv=train_data[\"dv\"])\n",
    "    model_obj = train_and_predict(\n",
    "        train_data[\"x\"], train_data[\"y\"], test_data[\"x\"], test_data[\"y\"]\n",
    "    )\n",
    "    if save_model:\n",
    "        save_pickle(f\"{local_storage}/model.pickle\", model_obj)\n",
    "    if save_dv:\n",
    "        save_pickle(f\"{local_storage}/dv.pickle\", train_data[\"dv\"])\n",
    "\n",
    "\n",
    "def batch_inference(\n",
    "    input_path, dv=None, model=None, local_storage=Config.LOCAL_STORAGE\n",
    "):\n",
    "    \"\"\"\n",
    "    Load model and dictvectorizer from folder\n",
    "    Transforms input data with dictvectorizer\n",
    "    Predict values using loaded model\n",
    "    :return array of predictions\n",
    "    \"\"\"\n",
    "    if not dv:\n",
    "        dv = load_pickle(f\"{local_storage}/dv.pickle\")\n",
    "    data = process_data(input_path, dv, with_target=False)\n",
    "    if not model:\n",
    "        model = load_pickle(f\"{local_storage}/model.pickle\")[\"model\"]\n",
    "    return predict_duration(data[\"x\"], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few popular workflow orchestrators :\n",
    "\n",
    "- Apache Airflow : Open-source platform, used for scheduling and managing workflows, has a large and active community.\n",
    "\n",
    "- Prefect : Open-source platform, designed to be highly flexible, easily deployable and scalable, offers a Python API.\n",
    "\n",
    "- Flyte : Open-source platform, unified platform for workflow management across cloud and on-premises environments. Also provides a Python API \n",
    "\n",
    "- AWS Step Functions : Serverless workflow service offered by Amazon Web Services, supports building and executing workflows with multiple steps.\n",
    "\n",
    "- Zapier : Web-based platform that provides a visual interface for connecting different web applications, does not require coding knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [Little Orchestrators Benchmark](https://miro.medium.com/max/1400/1*b6CAci-A4TfuYwM9coY6nw.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow orchestration with prefect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Version** : \n",
    "> This module has been created using Prefect 2.7.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Prefect concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefect uses python to build Jobs using functions decorators. \n",
    "As long as your main workflow function is decorated, any run of such flow becomes observable from the Prefect UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic concepts : \n",
    "- **Tasks in prefect** : They are units of work written in python. A task is a function decorated with the @task Prefect decorator. They can only be called in flows.\n",
    "- **Flows** : They are dags that represents a group of interdependant tasks. \n",
    "- **Engine** : This is what define where to run flows. This is where we manage workload distribution. In this course, we only use local machine.\n",
    "- **State** : They are prefect objects returned by flows. Contain informations about flows and data.\n",
    "\n",
    "Deployment concepts : \n",
    "- **Deployment object** : These are prefect entities that the api can understand for scheduling, auto-runs etc.\n",
    "- **Work queue** : These are created after a deployment have been applied. It lists all the upcomming runs and their state. (scheduled, running ...)\n",
    "- **Agent** : These are responsable of pulling the flows from work queues for run a the right time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Prefect UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps :\n",
    "\n",
    "- Set an API URL for your local server to make sure that your workflow will be tracked by this specific instance : \n",
    "    - `prefect config set PREFECT_API_URL=http://0.0.0.0:4200/api`\n",
    "- Start a local prefect server : `prefect orion start --host 0.0.0.0`\n",
    "\n",
    "Prefect database is stored at `~/.prefect/orion.db`. If you want to reset the database, run `prefect orion database reset`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! prefect config set PREFECT_API_URL=http://0.0.0.0:4200/api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! prefect orion start --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First flow : Processing flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to configure tasks and flows behavior using arguments in the decorators : \n",
    "- name, tags\n",
    "- version\n",
    "- retries on failure\n",
    "- etc ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(name=\"failure_task\", tags=[\"fails\"], retries=3, retry_delay_seconds=60)\n",
    "def failure():\n",
    "    print(\"running\")\n",
    "    if random.randint(1, 10) % 2 == 0:\n",
    "        raise ValueError(\"bad code\")\n",
    "\n",
    "\n",
    "@flow(name=\"failure_flow\", version=\"1.0\")\n",
    "def test_failure():\n",
    "    failure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_failure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create processing flows using the functions above and prefect : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(name=\"load_data\", tags=[\"preprocessing\"], retries=2, retry_delay_seconds=60)\n",
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "\n",
    "@task(name=\"compute_duration\", tags=[\"preprocessing\"])\n",
    "def compute_target(\n",
    "    df: pd.DataFrame,\n",
    "    pickup_column: str = \"tpep_pickup_datetime\",\n",
    "    dropoff_column: str = \"tpep_dropoff_datetime\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute the trip duration in minutes based\n",
    "    on pickup and dropoff time\n",
    "    \"\"\"\n",
    "    df[\"duration\"] = df[dropoff_column] - df[pickup_column]\n",
    "    df[\"duration\"] = df[\"duration\"].dt.total_seconds() / 60\n",
    "    return df\n",
    "\n",
    "\n",
    "@task(name=\"filter_outliers\", tags=[\"preprocessing\"])\n",
    "def filter_outliers(\n",
    "    df: pd.DataFrame, min_duration: int = 1, max_duration: int = 60\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove rows corresponding to negative/zero\n",
    "    and too high target' values from the dataset\n",
    "    \"\"\"\n",
    "    return df[df[\"duration\"].between(min_duration, max_duration)]\n",
    "\n",
    "\n",
    "@task(name=\"encode_cat_cols\", tags=[\"preprocessing\"])\n",
    "def encode_categorical_cols(\n",
    "    df: pd.DataFrame, categorical_cols: List[str] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Takes a Pandas dataframe and a list of categorical\n",
    "    column names, and returns dataframe with\n",
    "    the specified columns converted to categorical data type\n",
    "    \"\"\"\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = Config.CATEGORICAL_VARS\n",
    "    df[categorical_cols] = df[categorical_cols].fillna(-1).astype(\"int\")\n",
    "    df[categorical_cols] = df[categorical_cols].astype(\"str\")\n",
    "    return df\n",
    "\n",
    "\n",
    "@task(name=\"extract_x_y\", tags=[\"preprocessing\"])\n",
    "def extract_x_y(\n",
    "    df: pd.DataFrame,\n",
    "    categorical_cols: List[str] = None,\n",
    "    dv: DictVectorizer = None,\n",
    "    with_target: bool = True,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Turns lists of mappings (dicts of feature names to feature values)\n",
    "    into sparse matrices for use with scikit-learn estimators\n",
    "    using Dictvectorizer object.\n",
    "    :return The sparce matrix, the target' values if needed and the\n",
    "    dictvectorizer object.\n",
    "    \"\"\"\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = Config.CATEGORICAL_VARS\n",
    "    dicts = df[categorical_cols].to_dict(orient=\"records\")\n",
    "\n",
    "    y = None\n",
    "    if with_target:\n",
    "        if dv is None:\n",
    "            dv = DictVectorizer()\n",
    "            dv.fit(dicts)\n",
    "        y = df[\"duration\"].values\n",
    "\n",
    "    x = dv.transform(dicts)\n",
    "    return {\"x\": x, \"y\": y, \"dv\": dv}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flow(name=\"Data processing\", retries=1, retry_delay_seconds=30)\n",
    "def process_data(path: str, dv=None, with_target: bool = True) -> dict:\n",
    "    \"\"\"\n",
    "    Load data from a parquet file\n",
    "    Compute target(duration column) and apply threshold filters (optional)\n",
    "    Turn features to sparce matrix\n",
    "    :return The sparce matrix, the target' values and the\n",
    "    dictvectorizer object if needed.\n",
    "    \"\"\"\n",
    "    df = load_data(path)\n",
    "    if with_target:\n",
    "        df1 = compute_target(df)\n",
    "        df2 = filter_outliers(df1)\n",
    "        df3 = encode_categorical_cols(df2)\n",
    "        return extract_x_y(df3, dv=dv)\n",
    "    else:\n",
    "        df1 = encode_categorical_cols(df)\n",
    "        return extract_x_y(df1, dv=dv, with_target=with_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "res = process_data(Config.TRAIN_DATA)\n",
    "\n",
    "res_without_y = process_data(\n",
    "    path=Config.INFERENCE_DATA, dv=res[\"dv\"], with_target=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task(name=\"Train model\", tags=[\"Model\"])\n",
    "def train_model(x_train: csr_matrix, y_train: np.ndarray) -> LinearRegression:\n",
    "    \"\"\"Train and return a linear regression model\"\"\"\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(x_train, y_train)\n",
    "    return lr\n",
    "\n",
    "\n",
    "@task(name=\"Make prediction\", tags=[\"Model\"])\n",
    "def predict_duration(input_data: csr_matrix, model: LinearRegression) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Use trained linear regression model\n",
    "    to predict target from input data\n",
    "    :return array of predictions\n",
    "    \"\"\"\n",
    "    return model.predict(input_data)\n",
    "\n",
    "\n",
    "@task(name=\"Evaluation\", tags=[\"Model\"])\n",
    "def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Calculate mean squared error for two arrays\"\"\"\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "\n",
    "@task(name=\"Load\", tags=[\"Serialize\"])\n",
    "def load_pickle(path: str):\n",
    "    with open(path, \"rb\") as f:\n",
    "        loaded_obj = pickle.load(f)\n",
    "    return loaded_obj\n",
    "\n",
    "\n",
    "@task(name=\"Save\", tags=[\"Serialize\"])\n",
    "def save_pickle(path: str, obj: dict):\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@flow(name=\"Model initialisation\")\n",
    "def train_and_predict(x_train, y_train, x_test, y_test) -> dict:\n",
    "    \"\"\"Train model, predict values and calculate error\"\"\"\n",
    "    model = train_model(x_train, y_train)\n",
    "    prediction = predict_duration(x_test, model)\n",
    "    mse = evaluate_model(y_test, prediction)\n",
    "    return {\"model\": model, \"mse\": mse}\n",
    "\n",
    "\n",
    "@flow(name=\"Example Machine learning workflow\", retries=1, retry_delay_seconds=30)\n",
    "def complete_ml(\n",
    "    train_path: str,\n",
    "    test_path: str,\n",
    "    save_model: bool = True,\n",
    "    save_dv: bool = True,\n",
    "    local_storage: str = Config.LOCAL_STORAGE,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load data and prepare sparse matrix (using dictvectorizer) for model training\n",
    "    Train model, make predictions and calculate error\n",
    "    Save model and dictvectorizer to a folder in pickle format\n",
    "    :return none\n",
    "    \"\"\"\n",
    "    if not os.path.exists(local_storage):\n",
    "        os.makedirs(local_storage)\n",
    "\n",
    "    train_data = process_data(train_path)\n",
    "    test_data = process_data(test_path, dv=train_data[\"dv\"])\n",
    "    model_obj = train_and_predict(\n",
    "        train_data[\"x\"], train_data[\"y\"], test_data[\"x\"], test_data[\"y\"]\n",
    "    )\n",
    "    if save_model:\n",
    "        save_pickle(f\"{local_storage}/model.pickle\", model_obj)\n",
    "    if save_dv:\n",
    "        save_pickle(f\"{local_storage}/dv.pickle\", train_data[\"dv\"])\n",
    "\n",
    "\n",
    "@flow(name=\"Batch inference\", retries=1, retry_delay_seconds=30)\n",
    "def batch_inference(\n",
    "    input_path, dv=None, model=None, local_storage=Config.LOCAL_STORAGE\n",
    "):\n",
    "    \"\"\"\n",
    "    Load model and dictvectorizer from folder\n",
    "    Transforms input data with dictvectorizer\n",
    "    Predict values using loaded model\n",
    "    :return array of predictions\n",
    "    \"\"\"\n",
    "    if not dv:\n",
    "        dv = load_pickle(f\"{local_storage}/dv.pickle\")\n",
    "    data = process_data(input_path, dv, with_target=False)\n",
    "    if not model:\n",
    "        model = load_pickle(f\"{local_storage}/model.pickle\")[\"model\"]\n",
    "    return predict_duration(data[\"x\"], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_ml(Config.TRAIN_DATA, Config.TEST_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_inference(Config.INFERENCE_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deployments : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefect deployment objects are instances that are used by the prefect API tu understand scheduling requirements. \\\n",
    "A flow can be used in multiple deployment objects, but a deployment object is associated to a unique flow. \\\n",
    "It creates work queues and agent that manages the runs.\n",
    "\n",
    "There is two types of scheduling that can be used with prefect : \n",
    "- cron scheduling : define runs dates based on a cron expression. e.g. : `\"0 0 * * 0\"` (every sunday at 00:00)\n",
    "- interval scheduling : define runs interval in minutes/seconds/..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_deployment_every_sunday = await Deployment.build_from_flow(\n",
    "    name=\"Model training Deployment\",\n",
    "    flow=complete_ml,\n",
    "    version=\"1.0\",\n",
    "    tags=[\"model\"],\n",
    "    schedule=CronSchedule(cron=\"0 0 * * 0\"),\n",
    "    apply=True,\n",
    "    entrypoint=\"/app/lib/prefect_workflows.py:complete_ml\",\n",
    "    parameters={\"train_path\": Config.TRAIN_DATA, \"test_path\": Config.TEST_DATA},\n",
    ")\n",
    "\n",
    "\n",
    "inference_deployment_every_minute = await Deployment.build_from_flow(\n",
    "    name=\"Model Inference Deployment\",\n",
    "    flow=batch_inference,\n",
    "    version=\"1.0\",\n",
    "    tags=[\"inference\"],\n",
    "    schedule=IntervalSchedule(interval=60),\n",
    "    apply=True,\n",
    "    entrypoint=\"/app/lib/prefect_workflows.py:batch_inference\",\n",
    "    parameters={\"input_path\": Config.INFERENCE_DATA},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A prefect agent is needed to pull the works and run the flows at the right time/interval.\n",
    "Start one with : \n",
    "```\n",
    "prefect agent start default\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! prefect agent start default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Open Discussion : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Discuss a global vision of how to implement pipelines triggering (by action, not time)\n",
    "- Discuss a way to implement it to the NYC use case. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "6008d53778f36635ba13ef5eb2b9da68b78b7e259c3135e672f352dc09e97e14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
